---
title: k8s-lessons
slug: /2024/k8s-lessons
date:  2024-09-15
unlisted: true
---




## 《深入剖析Kubernetes》


[深入剖析Kubernetes](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90Kubernetes)






## 《Kubernetes入门实战课》

[Kubernetes入门实战课](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/Kubernetes%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98%E8%AF%BE)



## 《Kubernetes 实践入门指南》

[Kubernetes 实践入门指南](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/Kubernetes%20%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97)




## 《Kubernetes 从上手到实践》

[Kubernetes 从上手到实践](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/Kubernetes%20%E4%BB%8E%E4%B8%8A%E6%89%8B%E5%88%B0%E5%AE%9E%E8%B7%B5)





```markdown
正是由于这样的定位，Borg可以说是Google最不可能开源的一个项目。而幸运的是，得益于Docker项目和容器技术的风靡，它却终于得以以另一种方式与开源社区见面，这个方式就是Kubernetes项目。

所以，相比于“小打小闹”的Docker公司、“旧瓶装新酒”的Mesos社区，Kubernetes项目从一开始就比较幸运地站上了一个他人难以企及的高度：在它的成长阶段，这个项目每一个核心特性的提出，几乎都脱胎于Borg/Omega系统的设计与经验。更重要的是，这些特性在开源社区落地的过程中，又在整个社区的合力之下得到了极大的改进，修复了很多当年遗留在Borg体系中的缺陷和问题。

所以，尽管在发布之初被批评是“曲高和寡”，但是在逐渐觉察到Docker技术栈的“稚嫩”和Mesos社区的“老迈”之后，这个社区很快就明白了：Kubernetes项目在Borg体系的指导下，体现出了一种独有的“先进性”与“完备性”，而这些特质才是一个基础设施领域开源项目赖以生存的核心价值。

为了更好地理解这两种特质，我们不妨从Kubernetes的顶层设计说起。

首先，Kubernetes项目要解决的问题是什么？

编排？调度？容器云？还是集群管理？

实际上，这个问题到目前为止都没有固定的答案。因为在不同的发展阶段，Kubernetes需要着重解决的问题是不同的。

```















---

- [1.深入 k8s：k8s 部署&在 k8s 中运行第一个程序 -](https://www.luozhiyun.com/archives/314)
- [2.深入 k8s：Pod 对象中重要概念及用法 -](https://www.luozhiyun.com/archives/318)
- [3.深入 k8s：Deployment 控制器 -](https://www.luozhiyun.com/archives/328)
- [4.深入 k8s：持久卷 PV、PVC 及其源码分析 -](https://www.luozhiyun.com/archives/335)
- [5.深入 k8s：StatefulSet 控制器及源码分析 -](https://www.luozhiyun.com/archives/342)
- [6.深入 k8s：守护进程 DaemonSet 及源码分析 -](https://www.luozhiyun.com/archives/346)
- [7.深入 k8s：任务调用 Job 与 CronJob 及源码分析 -](https://www.luozhiyun.com/archives/359)
- [8.深入 k8s：资源控制 Qos 和 eviction 及其源码分析 -](https://www.luozhiyun.com/archives/363)
- [9.深入 k8s：调度器及其源码分析 -](https://www.luozhiyun.com/archives/366)
- [10.深入 k8s：调度的优先级及抢占机制源码分析 -](https://www.luozhiyun.com/archives/370)
- [11.深入 k8s：kubelet 工作原理及其初始化源码分析 -](https://www.luozhiyun.com/archives/373)
- [12.深入 k8s：kubelet 创建 pod 流程源码分析 -](https://www.luozhiyun.com/archives/378)
- [13.深入 k8s：Pod 水平自动扩缩 HPA 及其源码分析 -](https://www.luozhiyun.com/archives/381)
- [14.深入 k8s：kube-proxy ipvs 及其源码分析 -](https://www.luozhiyun.com/archives/385)
- [15.深入 k8s：Event 事件处理及其源码分析 -](https://www.luozhiyun.com/archives/388)
- [16.深入 k8s：Informer 使用及其源码分析 -](https://www.luozhiyun.com/archives/391)




```markdown
简述Kubernetes Scheduler作用及实现原理

Kubernetes Scheduler是负责Pod调度的重要功能模块，Kubernetes Scheduler在整个系统中承担了“承上启下”的重要功能，“承上”是指它负责接收Controller Manager创建的新Pod，为其调度至目标Node；“启下”是指调度完成后，目标Node上的kubelet服务进程接管后继工作，负责Pod接下来生命周期。

Kubernetes Scheduler的作用是将待调度的Pod（API新创建的Pod、Controller Manager为补足副本而创建的Pod等）按照特定的调度算法和调度策略绑定（Binding）到集群中某个合适的Node上，并将绑定信息写入etcd中。

在整个调度过程中涉及三个对象，分别是待调度Pod列表、可用Node列表，以及调度算法和策略。

Kubernetes Scheduler通过调度算法调度为待调度Pod列表中的每个Pod从Node列表中选择一个最适合的Node来实现Pod的调度。随后，目标节点上的kubelet通过API Server监听到Kubernetes Scheduler产生的Pod绑定事件，然后获取对应的Pod清单，下载Image镜像并启动容器。

简述Kubernetes Scheduler使用哪两种算法将Pod绑定到worker节点

Kubernetes Scheduler根据如下两种调度算法将 Pod 绑定到最合适的工作节点：

预选（Predicates）：输入是所有节点，输出是满足预选条件的节点。kube-scheduler根据预选策略过滤掉不满足策略的Nodes。如果某节点的资源不足或者不满足预选策略的条件则无法通过预选。如“Node的label必须与Pod的Selector一致”。

优选（Priorities）：输入是预选阶段筛选出的节点，优选会根据优先策略为通过预选的Nodes进行打分排名，选择得分最高的Node。例如，资源越富裕、负载越小的Node可能具有越高的排名。
```



```yaml
#- [ ] Kubernetes 从上手到实践
#- [ ] Kubernetes 实践入门指南
#- [ ] Kubernetes 入门实战课
#- [ ] 深入剖析 Kubernetes


```



### 实操一下“nginx ingress按照http header进行路由”


k8s 根据http header的version请求对应服务



怎么实现自动根据version请求对应的服务？除了用k8s之外，还有什么方法吗？



```markdown

项目API分版本，前端和后端肯定是通过v1, v2这样的标识来协作的，那借助k8s有没有什么直接根据不同标识，去请求不同应用的方法？具体怎么操作？

---

在 Kubernetes (k8s) 中，API 版本管理是一个重要的概念，它允许开发者在不同的环境或者部署中使用相同服务的不同版本。然而，k8s 本身并不直接支持基于 API 版本（如 v1, v2）来路由请求到不同的应用。k8s 的服务发现和路由通常是基于服务名和端口，而不是 API 版本。

不过，可以借助一些 k8s 生态系统中的工具和机制来实现类似的功能。以下是一些可能的解决方案：

1. **使用不同的服务名**：为每个版本的应用创建不同的 Kubernetes 服务，并使用不同的服务名。这样，客户端可以根据所需的 API 版本来选择调用哪个服务。

2. **Ingress 资源**：使用 Ingress 资源来管理外部访问，可以配置不同的路径或主机名来路由到不同版本的服务。例如，可以设置 `/api/v1/*` 路由到 v1 版本的服务，而 `/api/v2/*` 路由到 v2 版本的服务。

3. **金丝雀部署**：通过金丝雀部署，可以同时运行应用的多个版本，并逐步将流量从旧版本迁移到新版本。这可以通过修改 Deployment 的策略来实现。

4. **服务网格（如 Istio）**：服务网格提供了更细粒度的流量管理功能，可以根据请求的 API 版本来路由请求。Istio 允许开发者定义复杂的路由规则，包括基于 HTTP 头、路径等条件的路由。

5. **自定义应用程序逻辑**：在应用程序内部，可以根据请求的 URL 或者 HTTP 头中的版本信息来决定调用哪个版本的 API。

6. **API 网关**：使用 API 网关如 Kong、Amazon API Gateway 或者 Apigee 等，这些网关支持基于版本号的路由规则。

具体操作步骤会根据所选方案的不同而有所差异。例如，如果选择使用 Ingress 资源，你需要定义一个 Ingress 资源，并在其中配置相应的路径规则来路由请求到不同的服务。

请注意，这些方法并不是直接由 k8s 提供的，而是通过 k8s 生态系统中的其他工具和资源来实现的。每种方法都有其适用场景和配置方式，需要根据实际需求和环境来选择最合适的方案。[^3^][^10^]

```

[Kimi.ai - 帮你看更大的世界](https://kimi.moonshot.cn/chat/cp4n8bb3vddk0s9l7b80)


### [doc] K8S训练营

[K8S训练营](https://www.qikqiak.com/k8strain/)

---

```markdown
其中 Master 节点由三个独立的组件组成，它们分别是负责整个集群通信的 API 服务的 kube-apiserver、负责容器调度的 kube-scheduler 以及负责维护集群状态的 kube-controller-manager 组件。整个集群的数据都是通过 kube-apiserver 保存到 etcd 数据库中的，而其他所有组件的通信也都是通过 kube-apiserver 和 etcd 数据库进行通信的，都不会直接和 etcd 进行通信。

工作节点上最核心的组件就是 kubelet，当然还有底层的容器运行时，比如 Docker，其中 kubelet 就是主要来实现和底层的容器运行时进行通信的，这个通信的过程也被 Kubernetes 抽象成了一个 CRI（Container Runtime Interface）的远程调用接口，这个接口里面定义了容器运行时的所有标准操作，比如创建容器、删除容器等等，只是目前 kubelet 里面内置了 Docker 关于这个 CRI 实现的一个 shim，所以如果我们底层是 Docker 容器的话就不需要单独安装这个 CRI 的实现的组件了。其他的容器运行时是需要提供这样的一个接口实现组件的。所以对于 Kubernetes 来说他根本不关心你部署的到底是什么容器运行时，只要你这个容器运行时可以实现 CRI 接口就可以被 Kubernetes 来管理。

kubelet 的另外一个重要功能就是调用网络插件（CNI）和存储插件（CSI）为容器配置网络和存储功能，同样的 kubelet 也是把这两个重要功能通过接口暴露给外部了，所以如果我们想要实现自己的网络插件，只需要使用 CNI 就可以很方便的对接到 Kubernetes 集群当中去。
```


```markdown

Kubernetes 控制器
 ReplicaSet
 Deployment
 StatefulSet
 DaemonSet
 Job
 HPA
Kubernetes 配置管理
 ConfigMap
 Secret
 ServiceAccount
Kubernetes 安全
 RBAC
 Security Context
 准入控制器
Kubernetes 网络
 网络插件
 网络策略
 Service 服务
 Ingress
Kubernetes 调度器
 调度器介绍
 Pod 调度
Kubernetes 存储
 Local 本地存储
 Ceph 存储
 存储原理

```

这部分就是k8s Deployment文件的语法


```markdown
在我们的 Kubernetes 集群中为我们提供了 Job 和 CronJob 两种资源对象来应对我们的这种需求。
```

```markdown
apiVersion 与 kind 资源类型。
metadata 元数据： pod 名称、标签、注解。
spec 规格内部元件信息：容器镜像名称、卷等。

```

```markdown
metadata labels

基于组操作 pod 而非单个操作，metadata.labels 的 kv pair 标签可组织任何 k8s 资源，保存标识信息。

```



```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: hello
              image: alpine
              imagePullPolicy: IfNotPresent
              command:
                - /bin/sh
                - -c
                - date; echo 'Hello from the Kubernetes cluster';
          restartPolicy: OnFailure
```


```markdown
Pod：最基本的资源单元，代表一个或多个容器的运行实例。

Deployment：用于定义Pod的副本数量和更新策略的控制器。

Service：提供了一个稳定的网络端点，用于访问一组Pod。

Ingress：用于将外部流量路由到集群内部的Service。

ConfigMap：用于存储非敏感的配置数据。

Secret：用于存储敏感的配置数据，如密码、令牌等。

StatefulSet：用于管理有状态应用程序的控制器。

DaemonSet：确保每个节点上都运行一个Pod的控制器。
```


```markdown
k8s有哪几种kind？为啥最常见的是Deployment和Service搭配使用？

最常见的是Deployment和Service搭配使用的原因如下：

Deployment可以管理Pod的副本数量，并提供滚动更新和回滚功能。这使得应用程序的部署和升级变得更加容易和可靠。

Service为一组Pod提供了一个稳定的网络端点，使得其他应用程序或服务可以通过该Service进行访问。Service可以提供负载均衡和服务发现功能，使得应用程序更易于扩展和水平伸缩。

通过将Deployment和Service结合使用，可以实现应用程序的高可用性、可扩展性和网络访问。同时，它们也是Kubernetes中最常用的资源类型，因为它们提供了管理应用程序的基本功能。
```


```markdown
不同kind，对应的spec中的不同是吗？

---

是的，不同的kind在其对应的spec中会有不同的字段和配置选项。每个kind的spec定义了该资源的期望状态和配置信息。

例如，Deployment的spec中包含以下字段：
- replicas：定义Pod的副本数量。
- selector：用于选择要管理的Pod的标签。
- template：定义要创建的Pod的模板，包括容器镜像、环境变量、卷挂载等。

而Service的spec中包含以下字段：
- selector：定义要关联的Pod的标签选择器。
- ports：定义要暴露的端口号和协议。
- type：指定Service的类型，如ClusterIP、NodePort、LoadBalancer等。

不同的kind具有不同的spec字段，这些字段用于描述资源的属性和配置。Kubernetes根据每个kind的spec来创建、更新和管理对应的资源。因此，了解每个kind的spec是使用Kubernetes的关键一步。

```

总结一下：

最常用的就是Deployment搭配Service


---

[Kubernetes 的核心是 API 而非容器](https://mp.weixin.qq.com/s?__biz=Mzg5Mjc3MjIyMA==&mid=2247549266&idx=1&sn=2ee88bed9edfcd020f148ef3288b276b)

```markdown
Kubernetes API 扩展：CRD

提供一套跨厂商的标准结构和语义来声明核心基础设施（Pod/Service/Volume/ServiceAccount/……）， 是 Kubernetes 成功的基础。在此基础上，它又通过 CRD（Custom Resource Definition）， 将这个结构扩展到任何/所有基础设施资源。

CRD 在 1.7 引入，允许云厂商和开发者自己的服务复用 Kubernetes 的 spec/impl 编程框架。

1、有了 CRD，用户不仅能声明 Kubernetes API 预定义的计算、存储、网络服务，还能声明数据库、task runner、消息总线、数字证书……任何云厂商能想到的东西！

2、Operator Framework 以及 SIG API Machinery 等项目的出现，提供了方便地创建和管理这些 CRD 的工具，最小化用户工作量，最大程度实现标准化。

例如，Crossplane 之类的项目，将厂商资源 RDS 数据库、SQS queue 资源映射到 Kubernetes API，就像核心 Kubernetes controller 一样用自己的 controller 来管理网卡、磁盘等自定义资源。Google、RedHat 等 Kubernetes 发行商也在它们的基础 Kubernetes 发行版中包含越来越多的自定义资源类型。
```

```markdown
随着 Kubernetes 资源模型越来越广泛的传播，现在已经能够 用一组 Kubernetes 资源来描述一整个软件定义计算环境。就像用 docker run 可以启动单个程序一样，用 kubectl apply -f 就能部署和运行一个分布式应用， 而无需关心是在私有云还是公有云以及具体哪家云厂商上，Kubernetes 的 API 框架已经屏蔽了这些细节。
```

确实，只需要kubectl apply就能随意拉起任意想要的分布式应用



> ***直观类比：Kubernetes 是个数据库，CRD 是一张表，API 是 SQL***













## micro-service


[ServiceMesh 究竟解决什么问题？](https://mp.weixin.qq.com/s/R0FW9wBpDCXxYJthpTf3sA)

[从边车模式到 Service Mesh](https://mp.weixin.qq.com/s/JGu5O5YDkhjiuTOpigMaTA)

[Service Mesh Interface 详细介绍](https://mp.weixin.qq.com/s/2pAYAkyaNRyck3m6n7Xjng)

---

Istio

Istio 是 ServiceMesh 的产品化落地

- 帮助微服务之间建立连接，可以更好地管理和监控微服务
- 帮助微服务分层解耦，解耦后的 proxy 层能够更加专注于提供基础架构能力



---

- [Istio 分层架构？80% 的人有误解](https://mp.weixin.qq.com/s/1xoZounMG-azUqP564oppA)
- [Istio 监控详解](https://mp.weixin.qq.com/s/0NG85NnGSuGKxNUK1KdLNg)
- [基于 Go、gRPC 和 Protobuf 的微服务的 Istio 可观察性](https://mp.weixin.qq.com/s/sDMX7ICSasyzrFQVeK4vmw)
- [Istio 遥测和可观察性探索](https://mp.weixin.qq.com/s/2EJhxb5OapMIX5ExEs1J_Q)
- [Istio 流控，服务发现，负载均衡，核心流程是如何实现的？](https://mp.weixin.qq.com/s/LMtCFleBp1Si1iDHNbEBVQ)
- [Istio，灰度发布从未如此轻松！！！](https://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651962280&idx=1&sn=31e105ae7cc3ce20e9950ddfb020eea3&chksm=bd2d0e748a5a8762b16441fa7244f8ed79f33de74d158210a728cfcb1ee65010f2aa50c546ff&scene=21#wechat_redirect)
- [Istio 和 Linkerd 的 CPU 基准测试报告](https://mp.weixin.qq.com/s/bry4g11lacH1eyuh5uVcHw)


---

Envoy

Envoy 是调度 ServiceMesh 的一个组件

[Service Mesh：调度千军万马微服务，2.0 妥妥的 - 知乎](https://zhuanlan.zhihu.com/p/149804338)


```markdown
envoy和istio都是service mesh的解决方案吗？有啥区别？

---

是的，Envoy和Istio都是服务网格（Service Mesh）的解决方案。然而，它们在服务网格中扮演不同的角色，并提供不同的功能。

Envoy:
Envoy是一个高性能的、开源的边缘和服务代理，也是一个独立的组件。它由Lyft公司开发，并于2017年成为Cloud Native Computing Foundation（CNCF）的一个孵化项目。Envoy作为服务网格中的数据平面（Data Plane）组件，负责处理服务间的网络通信。Envoy的主要功能包括：
- 负载均衡：Envoy支持多种负载均衡算法和健康检查机制，可以在服务之间动态地分发流量。
- 服务发现：Envoy能够与服务注册中心集成，自动发现和管理后端服务的实例。
- 故障恢复：Envoy提供了故障检测和故障恢复机制，可以自动处理后端服务的故障。

Istio:
Istio是一个开源的、云原生的服务网格平台，由Google、IBM和Lyft等公司共同开发。Istio建立在Envoy代理之上，并提供了更高级的功能。Istio作为服务网格中的控制平面（Control Plane）组件，负责管理和配置服务代理（Envoy）以及服务间的通信。Istio的主要功能包括：
- 流量管理：Istio提供丰富的流量管理功能，包括智能路由、A/B测试、金丝雀发布等。
- 安全性：Istio提供了服务间的身份和认证机制，以及流量的加密和安全控制。
- 策略和配额：Istio允许定义细粒度的策略和配额，以控制服务之间的通信行为。
- 遥测和监控：Istio集成了流量追踪、指标收集和日志记录等功能，以便进行全面的监控和故障排查。

总结起来，Envoy是一个独立的边缘和服务代理，负责处理服务之间的网络通信；而Istio是一个构建在Envoy之上的服务网格平台，提供了更高级的控制平面功能，包括流量管理、安全性、策略和监控等，以帮助构建和管理复杂的微服务架构。

```

---

单体还是微服务这个问题，已经没啥好聊的了。大部分文章都是从正面说微服务如何如何好，但是之前有个文章“为什么游戏的服务端不用微服务架构”从反面聊这个事情，基本上列举了几种典型的不应该使用微服务架构的场景。

我们对微服务的常见刻板印象是“微服务就是银弹，是照妖镜，系统是否robust就看迁移性怎么样。只要用了微服务，team就可以随便使用各种语言来实现微服务的各种模块。部署简单，服务解藕，故障隔离”。其实这些刻板印象出了运维相关的是正确的，其他的都是错误的。现在我们对微服务祛魅以后，


---

从单体应用迁移到微服务，需要注意哪些问题？（单体应用迁移到微服务的注意事项和经验？）

- *先分离数据库，再分离服务*，数据模型能否彻底分开，决定了微服务的边界功能是否彻底划清。我们已经见过太多直接从服务分离而造成多次重构和返工的案例
- *对无法修改的遗留系统，使用 `绞杀者模式`*，也就是在遗留系统外面增加新功能做成微服务，而不是直接修改原有系统，逐步实现对老系统的替换
- *建立统一的日志规范*，规范整个系统而非微服务的日志体系，采用标准的日志格式非常便于后续的日志聚合检索，便于整体的视角分析、监控、查看系统
- *选择成熟框架*，比如 spring 家族、docker、k8s 等，不要自己造轮子


---

不要拆分代码仓库，不要拆微服务

使用 `monorepo`+`feature toggle`

Monorepo：所有的代码都在一个仓库里。这样就不存在不同模块的仓库有不同的版本问题。大家都是统一的一个版本。升级线上系统的过程拆分成：部署 + 发布，两个步骤。部署的时候，整个 Monorepo 的代码都部署到目标机器上了，但并不代表发布了。

Feature Toggle：特性开关来精确控制哪些逻辑分支被发布出去。这样部署就和发布解耦了。要灰度多少比例，可以精确控制。要一个特性开关的两个版本的逻辑分支共存，也可以实现。

[如何不 Review 每一行代码，同时保持代码不被写乱？](https://mp.weixin.qq.com/s?__biz=MzU3NzEwNjI5OA==&mid=2247484737&idx=1&sn=3bafa3d2288af3630d59814ebf73f760)


---


最精简的微服务架构需要哪些组件？



---




list-watch机制和informer模块


[K8s list-watch 机制和 Informer 模块 - Lain Blog](https://xuliangtang.github.io/posts/k8s-list-watch/)


[理解 K8S 的设计精髓之 List-Watch机制和Informer模块 - 知乎](https://zhuanlan.zhihu.com/p/59660536)

```markdown
最近想深入了解一下K8S的内部通信机制，因此读了几遍K8S的源码，感慨很深。至今清楚的记得，当了解到K8S组件之间仅采用HTTP 协议通信，没有依赖中间件时，我非常好奇它是如何做到的。

在K8S内部通信中，肯定要保证消息的实时性。之前以为方式有两种：

1、客户端组件(kubelet,scheduler,controller-manager等)轮询 apiserver
2、apiserver通知客户端。

如果采用轮询，势必会大大增加apiserver的压力，同时实时性很低。
如果apiserver主动发HTTP请求，又如何保证消息的可靠性，以及大量端口占用问题？

当阅读完list-watch源码后，先是所有的疑惑云开雾散，进而为K8S的设计理念所折服。List-watch是K8S统一的异步消息处理机制，保证了消息的实时性，可靠性，顺序性，性能等等，为声明式风格的API奠定了良好的基础，它是优雅的通信方式，是K8S 架构的精髓。
```

list-watch是k8s的异步消息处理机制


```markdown
我感觉k8s的list-watch机制和reactor线程模型的设计很类似，是吗？reactor就是用epoll监听fd是否有响应，如果有响应就把client连接放到一个FIFO队列里，epoll和队列都在主线程中，fork出n个子线程来接管各个fd。而list-watch则是reflector从API Server中通过watch得到资源状态变化，把数据塞到Delta FIFO队列中（reflector相当于producer），再由informer进行消费。我的理解正确吗？如果有不完整的地方，请指出

---

你的理解基本上是正确的！Kubernetes的`list-watch`机制和reactor线程模型确实具有相似之处。

在reactor模型中，主线程通过epoll监听文件描述符（fd）是否有响应。如果有响应，主线程会将连接放入一个FIFO队列中，然后通过fork创建多个子线程来处理这些连接。这种模型允许主线程专注于监听，而子线程负责处理具体的连接，提高了并发处理能力。

Kubernetes中的`list-watch`机制也类似，但是应用于资源的状态变化。Kubernetes中的Controller组件（如Reflector和Informer）会通过watch机制从API Server中获取资源的状态变化，然后将这些变化放入Delta FIFO队列中。Reflector相当于生产者，负责从API Server中获取资源状态变化，而Informer则是消费者，负责处理这些变化并采取相应的操作。

总之，这两种机制都采用了类似的生产者-消费者模型，通过使用队列将任务分离，提高了并发性和系统的可伸缩性。希望这能帮助到你，如果还有其他问题，请随时提问！

```

***list-watch机制可以理解为reactor线程模型***


---


```markdown
kind：对象的类别

metadata：元数据，如 Pod 的名称，以及标签 Label【用于识别一系列关联的 Pod，可以使用 Label Selector 来选择一组相同 label 的对象】

spec：希望 Pod 能达到的状态，在此体现了 Kubernetes 的声明式的思想，我们只需要定义出期望达到的状态，而不需要关心如何达到这个状态，这部分工作由 Kubernetes 来完成。这里我们定义了 Pod 中运行的容器列表，包括一个 nginx 容器，该容器对外暴露了 80 端口。
```

spec是希望pod能达到的状态

```markdown
4、Node
Node 是 Pod 真正运行的主机，可以是物理机，也可以是虚拟机。为了管理 Pod，每个 Node 节点上至少要运行 container runtime、kubelet 和 kube-proxy 服务。
```

```markdown
5、Deployment
Deployment 用于管理一个无状态应用，对应一个 Pod 的集群，每个 Pod 的地位是对等的，对 Deployment 来说只是用于维护一定数量的 Pod，这些 Pod 有着相同的 Pod 模板。

6、Service
Service 用于将一组 Pod 暴露为一个服务。

在 kubernetes 中，Pod 的 IP 地址会随着 Pod 的重启而变化，并不建议直接拿 Pod 的 IP 来交互。那如何来访问这些 Pod 提供的服务呢？使用 Service。Service 为一组 Pod（通过 labels 来选择）提供一个统一的入口，并为它们提供负载均衡和自动服务发现。
```

```markdown
7、Kubernetes 组件
etcd 保存了整个集群的状态；
apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制；
controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；
scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上；
kubelet 负责维护容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理；
Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI）；
kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡
```

```markdown
CRD【CustomResourceDefinition】
CRD 是 Kubernetes 为提高可扩展性，让开发者去自定义资源（如 Deployment，StatefulSet 等）的一种方法。

Operator=CRD+Controller。
```

```markdown
如何去实现一个 Controller 呢？

可以使用 Go 来实现，并且不论是参考资料还是开源支持都非常好，推荐有 Go 语言基础的优先考虑用client-go来作为 Kubernetes 的客户端，用KubeBuilder来生成骨架代码。一个官方的 Controller 示例项目是sample-controller。

对于 Java 来说，目前 Kubernetes 的 JavaClient 有两个，一个是 Jasery，另一个是Fabric8。后者要更好用一些，因为对 Pod、Deployment 都有 DSL 定义，而且构建对象是以 Builder 模式做的，写起来比较舒服。

Fabric8 的资料目前只有https://github.com/fabric8io/kubernetes-client，注意看目录下的 examples。

这些客户端本质上都是通过 REST 接口来与 Kubernetes API Server 通信的。

Controller 的逻辑其实是很简单的：监听 CRD 实例（以及关联的资源）的 CRUD 事件，然后执行相应的业务逻辑
```

---


*[APISIX Ingress 如何使用 cert-Manager 管理证书](https://mp.weixin.qq.com/s?__biz=MzI2ODAwMzUwNA==&mid=2649298306&idx=2&sn=508e6e79ffe925586e1f96b514eac8cc#rd)*

cert-manager的使用很简单，但是是怎么实现的呢？

```markdown
证书签发过程繁琐

上述内容中我并没有演示如何进行证书的签发，如果你对此感兴趣可以查看 OpenSSL 的文档[3] 。在证书的签发过程中，需要理解很多概念。而且签发过程都发生在 Kubernetes 集群之外，不能很好的通过“声明式”配置的方式来了解具体发生了什么事情。尤其是证书可以有多种不同的加密算法，各种不同的配置等。
所以如果使用默认的方式，只能最后将生成的证书和密钥存储在 Kubernetes 的 Secrets 中。

---

证书续签/重签繁琐

我们都知道证书是有过期时间的，在证书过期或者被吊销之前，必须准备好新的证书，并且保证其过期时间要晚于原证书的过期时间。
在 Kubernetes Secrets 中存储的证书，从这方面考虑的话有些不足：

- 不存在自动化的过期时间检查：也就是说，你可以在 Kubernetes 中存储任意的证书，无论该证书是否已经过期。

- 不存在无效数据的检查：也就是说，如果存储在 Kubernetes Secrets 中的数据是损坏的，或者是无效的，那么在 Kubernetes 中也不会有什么特殊的处理。
```

k8s提供ConfigMap和secret来存储数据，但是ConfigMap是明文，secret则适用于存放密文。但是证书的签发、续签、重签都很麻烦，并且直接存放在secret中仍然安全性不足（这也是为啥有vault这样的工具）。

```shell
kubectl get secrets moelove-tls -o jsonpath='{ .data.tls\.crt }' |base64 -d
```

可以通过这个命令直接拿到TLS证书的明文。


```markdown
cert-manager 通过 CRD 的方式进行开发和扩展，其添加和实现了 Issuers 和 ClusterIssuers 资源，代表证书的签发机构（CA）。

并且支持了多种内置类型，以及可以很方便的与外部组件进行集成，比如：

SelfSigned：自签证书
CA：提供 CA 进行签发
Vault：使用 Vault 进行签发
Venafi：使用 Venafi 进行签发
External：使用一些外部的组件进行签发，比如：
    - kms-issuer[4] ：使用 AWS KMS 签发
    - google-cas-issuer[5] ：使用 Google CAS 进行签发
    - [origin-ca-issuer][6]：使用 Cloudflare Origin CA[7] 进行签发
ACME（Automated Certificate Management Environment ）：自动化进行证书签发

通过这些组件可以非常方便的进行证书的签发。后续内容中将会以 Vault 为例进行具体介绍。
```

我们平时是把cert-manager直接集成到ingress，但是这里我们需要


---


[APISIX Ingress 如何支持自定义插件](https://mp.weixin.qq.com/s?__biz=MzI2ODAwMzUwNA==&mid=2649298220&idx=2&sn=e4cb9fac6198102caf95f4ca8c0d3c91)


- APISIX Ingress
- Ingress-NGINX
- easegress

这些gateway，sidecar, pipeline

[apache/apisix-ingress-controller: APISIX Ingress Controller for Kubernetes](https://github.com/apache/apisix-ingress-controller)

---

*[不确定K8S YAML文件是否符合最佳实践？这6个工具可以帮你！_软件工程_Rancher_InfoQ精选文章](https://www.infoq.cn/article/dzc6evcihvvzfmuxf62n)*

```markdown
Kubernetes YAML 文件静态检查的生态系统可以分为以下几类：

- API 验证器：这一类工具可以针对 Kubernetes API 服务器验证给定的 YAML manifest。
- 内置检查器：这一类工具捆绑了安全、最佳实践等方面的意见检查。
- 自定义验证器：这一类工具允许用几种语言编写自定义检查，如 Rego 和 Javascript。


在本文中，你将了解并比较六种不同的工具：

- Kubeval
- Kube-score
- Config-lint
- Copper
- Conftest
- Polaris

```

---




*[Kubernetes 微服务最佳实践 - This Cute World](https://thiscute.world/posts/kubernetes-best-practices/)*

这篇文章太干了，涉及以下方面：

- terminate pod时的具体流程，以及为什么因此报错502/504。如果使用sidecar作为gateway，应该怎么处理？
- HPA，HPA的扩容和缩容算法？HPA的期望值设置为多少合适？
- PDB
- 三种probe




```markdown
- 这里只讨论无状态服务，有状态服务不在讨论范围内
- 我们不使用 Deployment 的滚动更新能力，而是为每个服务的每个版本，都创建不同的 Deployment + HPA + PodDisruptionBudget，这是为了方便做金丝雀/灰度发布
- 我们的服务可能会使用 IngressController / Service Mesh 来进行服务的负载均衡、流量切分
```

***不要使用Deployment内置的rollingUpdate，如果想实现灰度发布，需要给每个服务的每个版本都创建不同的Deployment+HPA+PodDisruptionBudget***




---

k8s 实现金丝雀发布？

[k8s 如何完成金丝雀发布 - 知乎](https://zhuanlan.zhihu.com/p/263569381)

k8s 灰度发布怎么调整流量？怎么加入灰度标签？


---
